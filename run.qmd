---
title: `r Sys.Date()`
format: html
toc: true
---

TODO: stamp each render/run of this script with a UUID, and add log/<method>/<UUID> dirs
TODO: according to ?system, system2() is better, so switch to that eventually I guess. More portable. 
TODO: replace all the "r"s or "c"s with "i"s or "j"s in apply functions like mapply(1:nrow(x), 1:ncol(x), function(r,c) {...})

# Lib
```{r}
#| output: false
library(data.table)
library(SeqArray)
library(SNPRelate)
```
```{r}
"%ni%" <- Negate("%in%")
```

# Load Configs
```{r}
misc_config <- list()
              source("config/config.misc")
data_config <- fread("config/config.data")
loci_config <- fread("config/config.loci")
anno_config <- fread("config/config.anno")
  ld_config <- fread("config/config.ld"  )
chr_nm_map <- fread("config/ucsc_chr_names_map.txt")
```
TODO: printout "stamp" of configs s.t. in the rendered doc, it is clear what was run.

## Hands-free hardcoded reference panels
```{r}
if(misc_config$ref_panel == "1kG") {
  misc_config$ref_panel_ref_genome <- "hg19"
  misc_config$ref_panel <- paste0("http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/ALL.chr",1:22,".phase3_shapeit2_mvncall_integrated_v5b.20130502.genotypes.vcf.gz")
  misc_config$ref_panel <- c(misc_config$ref_panel, "http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/ALL.chrX.phase3_shapeit2_mvncall_integrated_v1c.20130502.genotypes.vcf.gz")
  #misc_config$ref_panel <- c(misc_config$ref_panel, "http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/ALL.chrY.phase3_integrated_v2b.20130502.genotypes.vcf.gz") # TODO: different samples?
  #misc_config$ref_panel <- c(misc_config$ref_panel, "http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/ALL.chrMT.phase3_callmom-v0_4.20130502.genotypes.vcf.gz") # TODO: different samples?
  misc_config$sample_info_file <- "http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/integrated_call_samples_v3.20130502.ALL.panel"
  misc_config$sample_id_field <- "sample"
  misc_config$sample_ancestry_field <- "super_pop"
} else
if(misc_config$ref_panel == "1kG_30x") {
  misc_config$ref_panel_ref_genome <- "hg38"
  misc_config$ref_panel <- paste0("http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/1000G_2504_high_coverage/working/20220422_3202_phased_SNV_INDEL_SV/1kGP_high_coverage_Illumina.chr",1:22,".filtered.SNV_INDEL_SV_phased_panel.vcf.gz")
  misc_config$ref_panel <- c(misc_config$ref_panel, "http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/1000G_2504_high_coverage/working/20220422_3202_phased_SNV_INDEL_SV/1kGP_high_coverage_Illumina.chrX.filtered.SNV_INDEL_SV_phased_panel.v2.vcf.gz")
  misc_config$sample_info_file <- "http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/1000G_2504_high_coverage/20130606_g1k_3202_samples_ped_population.txt"
  misc_config$sample_id_field <- "SampleID"
  misc_config$sample_ancestry_field <- "Superpopulation"
} else
if(misc_config$ref_panel == "gnomAD_1kG+HGDP") {
  misc_config$ref_panel_ref_genome <- "hg38"
  misc_config$ref_panel <- paste0("https://gnomad-public-us-east-1.s3.amazonaws.com/release/3.1.2/vcf/genomes/gnomad.genomes.v3.1.2.hgdp_tgp.chr",c(1:22,"X","Y"),".vcf.bgz") # Only the Amazon links work, otherwise corrupts halfway through vcfs2Gds().

  # gnomAD sample info file has wacky JSON stuff. Extract what we need to a temporary file.
  sample_info <- fread(cmd="curl https://gnomad-public-us-east-1.s3.amazonaws.com/release/3.1.2/vcf/genomes/gnomad.genomes.v3.1.2.hgdp_1kg_subset_sample_meta.tsv.bgz | zcat")
  sample_info$pop <- sapply(sample_info$gnomad_population_inference, function(x) ifelse(is.na(x), NA, jsonlite::fromJSON(x)$pop))
  sample_info[pop=="nfe", pop:="eur"]
  if(misc_config$gnomad_count_fin_as_eur) sample_info[pop=="fin", pop:="eur"]
  # gnomAD also recommends using this "high_quality" sample filter: https://gnomad.broadinstitute.org/news/2021-10-gnomad-v3-1-2-minor-release/
  sample_info <- sample_info[high_quality==TRUE]
  fwrite(sample_info[,.(s, pop)], paste0(misc_config$scratch_dir,"gnomAD_sample_info.txt"))

  misc_config$sample_info_file <- paste0(misc_config$scratch_dir,"gnomAD_sample_info.txt")
  misc_config$sample_id_field <- "s"
  misc_config$sample_ancestry_field <- "pop"
}
```

## Validate input
```{r}
# TODO:
# The rest of the code is cleaner under the assumption all input datasets are valid, and error messages early save the user time.
# data
  # data files exist (and maybe check if they're valid too? Can use fread(...,nrows=3) to quickly peek)
  # all cols required by all misc_config$fm_methods is there are specified for every data file
  # valid reference genomes
  # every dataset's ancestry is a valid ancestry in the reference panel too
    # robust to whether these ancestry names are capitalized or not
# loci: make sure valid reference genomes
# anno
  # Make sure no "+"s in anno names b/c will mess w/ fgwas.
  # Check anno files all exist (and are proper bed format too? maybe nah)
# misc
  # Ref panel
    # Check files exist if custom reference panel
    # Sample info
      # Check that sample_ancestry_field exists in either the sample_info_file or VCF/BCF/GDS file(s)
      # Check that sample_id_field exists in sample_info_file if given
      # Check that sample_info IDs match ref_panel (should do this even for hard-coded ref_panels for uniformity)
    # If pre-computed LD
      # Make sure no NAs or NaNs
      # Warn if some loci in config.ld & config.loci don't match
      # Warn if some ancestries in config.ld & config.data don't match
if(!dir.exists(misc_config$scratch_dir)) stop(paste0("Scratch directory ", misc_config$scratch_dir, " does not exist. Please create it, or change your desired scratch directory in config.misc."))
```

## `liftOver` loci and annotations
Lift loci and annotations to the data's reference genome(s). Also lift loci to the reference panel's reference genome, for LD calculation later.\
Better not to lift the sumsmary stats if it can be avoided. They are likely the biggest files, especially before we have had the chance to subset them to only the necessary variants.\
In addition, it is **not** recommended (source [1](http://genome.ucsc.edu/FAQ/FAQreleases.html#snpConversion), [2](https://groups.google.com/a/soe.ucsc.edu/g/genome/c/wkgf5DXpwEc/m/wDpsjyDVCQAJ), [3](https://annovar.openbioinformatics.org/en/latest/articles/dbSNP/)) to `liftOver` individual variants.\
Lifting variants by matching their rsID across reference genomes is *more* accurate, but even rsIDs can be ambiguous ([source](https://gnomad.broadinstitute.org/help/why-is-this-variant-linked-to-the-wrong-dbsnp-rsid)).

### Convert to UCSC naming scheme
`liftOver` uses the UCSC naming scheme, we need to too.\
[AFAIK, b37, hg19, and GRCh37 sequences are equivalent except in mitochondria and some contigs (exactly what is different nobody seems to know... see this somewhat dubious [source](https://gatk.broadinstitute.org/hc/en-us/articles/360035890711-GRCh37-hg19-b37-humanG1Kv37-Human-Reference-Discrepancies)). Beyond that they have different chromosome naming conventions: hg19 uses "chr#" instead of just "#". AFAIK GRCh38 and hg38 are fully equivalent though.]{.aside}
```{r}
#| output: false
# TODO: could be more elegant to use a reference genomes synonyms file or something
lapply(list(data_config, loci_config, anno_config), function(config_file) {
  config_file[tolower(ref_genome) %in% c("grch36"       ), ref_genome := "hg18"]
  config_file[tolower(ref_genome) %in% c("grch37", "b37"), ref_genome := "hg19"]
  config_file[tolower(ref_genome) %in% c("grch38"       ), ref_genome := "hg38"]
  config_file[, ref_genome := factor(ref_genome)]
})
if(tolower(misc_config$ref_panel_ref_genome) %in% c("grch36"       )) misc_config$ref_panel_ref_genome <- "hg18"
if(tolower(misc_config$ref_panel_ref_genome) %in% c("grch37", "b37")) misc_config$ref_panel_ref_genome <- "hg19"
if(tolower(misc_config$ref_panel_ref_genome) %in% c("grch38"       )) misc_config$ref_panel_ref_genome <- "hg38"

# NOTE: yes future self, this is correct, I'm 100% sure. Leaving this note here so you don't second guess. Rm me once you're comfortable with factors.
# TODO: would be good if it didn't turn already-properly-named chrs to NA. I.e. should be idempotent
loci_config$chr <- chr_nm_map$to[match(loci_config$chr, chr_nm_map$from)]
loci_config$chr <- factor(loci_config$chr, levels=unique(chr_nm_map$to))
```

### `liftOver`
```{r}
if(!dir.exists("out/liftover")) dir.create("out/liftover", recursive=T)
source("inc/liftOver.R")

loci_configs <-
lapply(union(data_config$ref_genome, misc_config$ref_panel_ref_genome), function(dg) {
  loci_config_bed_format <- data.table(
    chrom      = loci_config$chr,
    chromStart = loci_config$pos_min,
    chromEnd   = loci_config$pos_max,
    name       = loci_config$locus
  )

  loci_config_lifted <- do.call(rbind,
  lapply(unique(loci_config$ref_genome), function(lg) {
    loci_of_a_ref_genome_bed <- loci_config_bed_format[loci_config$ref_genome == lg] 
    if(dg != lg)
      liftOver(loci_of_a_ref_genome_bed,
               from_build = lg, to_build = dg,
               liftover_bin="third_party/liftover/liftOver")
    else
      loci_of_a_ref_genome_bed
  })
  )

  setnames(setDT(loci_config_lifted), c("chr","pos_min","pos_max","locus"))
  loci_config_lifted$chr   <- factor(loci_config_lifted$chr, levels=unique(chr_nm_map$to))
  loci_config_lifted$locus <- factor(loci_config_lifted$locus, levels=unique(loci_config$locus))
  fwrite(loci_config_lifted, paste0("out/liftover/config.loci.",dg), sep=' ')
  loci_config_lifted
})
names(loci_configs) <- union(data_config$ref_genome, misc_config$ref_panel_ref_genome)

anno_configs <- 
lapply(unique(data_config$ref_genome), function(dg) {
  lifted_anno_config <- copy(anno_config)
  sapply(1:nrow(anno_config), function(i) {
    ag <- anno_config[i,ref_genome]
    if(dg != ag) {
      lifted_bed_file <- paste0("out/liftover/",anno_config[i,name],"-lifted_to_",dg,".bed")
      liftOver(anno_config[i,bed_file],
               out_file = lifted_bed_file, 
               from_build = ag, to_build = dg,
               liftover_bin="third_party/liftover/liftOver")

      lifted_anno_config[i, `:=`(ref_genome = dg,
                                 bed_file = lifted_bed_file)]
    }
  })
  fwrite(lifted_anno_config, paste0("out/liftover/config.anno.",dg), sep=' ')
  lifted_anno_config
})
names(anno_configs) <- unique(data_config$ref_genome)
```


# Load data
## GWAS summary stats
```{r}
#| output: false
sumstats <- lapply(1:nrow(data_config), function(r) {
  loci <- loci_configs[[ data_config[r,ref_genome] ]] # Loci with coords lifted to the current data file's ref genome

  cols <- data_config[r, c( chr_col, pos_col, rs_id_col, effect_allele_col, other_allele_col, eaf_col, b_col, se_col, z_col, p_col, n_col)]
  cols <- setNames(cols, c("chr",   "pos",   "rs_id",   "a1",              "a0",             "eaf",   "b",   "se",   "z",   "p",   "n"   ))
  cols <- cols[!is.na(cols)]

  sumstat <- fread(data_config[r,filepath])
  sumstat <- sumstat[, ..cols]
  setnames(sumstat, names(cols))

  sumstat$chr <- chr_nm_map$to[match(sumstat$chr, chr_nm_map$from)]
  sumstat$chr <- factor(sumstat$chr, levels=unique(chr_nm_map$to))

  sumstat <- do.call(rbind, lapply(1:nrow(loci), function(i) {
    sumstat[ chr == loci$chr[i]     &
             pos >= loci$pos_min[i] &
             pos <= loci$pos_max[i] ][, locus := loci$locus[i]]
  }))
  if("eaf" %in% names(sumstat)) sumstat <- sumstat[eaf > 0 & eaf < 1]

  sumstat[,`:=`(a0=toupper(a0), a1=toupper(a1))]

  # Infer some columns from the others if possible. For axample, z-score can be calculated from β and SE.
  if(data_config[r, is.na(z_col) & !is.na(b_col) & !is.na(se_col)]) sumstat[, z := b/se]
  # TODO: possible to reverse-engineer N from p/z/se/af if n_col not given?

  # TODO: meh, maybe just compare w/ $chr $pos etc. directly instead of creating these IDs?
    # No, ID col is good b/c passing ids of this format is the clearest interface for functions like vcfs2Gds and ldCalc
    # But then again, could just reconstruct the ids e/t time? No, too much, b/c'd have to do the Reduce around the paste(), too much.
  #if(data_config[r, !is.na(chr_col) & !is.na(pos_col) & !is.na(other_allele_col) & !is.na(effect_allele_col)])
    #sumstat$chrpos_id <- paste0(sumstat$chr,":",sumstat$pos,"_",sumstat$a0,"_",sumstat$a1) # NOTE: using chr:pos_ref_alt instead of all ":"s, but doesn't matter, split by both : and _ at all times anyways.

  # TODO: add support for chrpos_id_col. Infer chr, pos, a0, a1 cols if chrpos ids are given

  sumstat
})
names(sumstats) <- basename(data_config$filepath)
```


## Reference panel
```{r}
if(any(misc_config$ref_panel != "my_ld")) { # TODO: replace this and future such checks w/ a proper ref_panel_is_ld variable or s/t
  regions_bed_format <- loci_configs[[misc_config$ref_panel_ref_genome]][,.(chr,pos_min,pos_max)]
  # TODO: if ref and ALL sumstats are of the same build, could be more specific and give the chr,pos-1,pos of the variants in the sumstats as regions instead. Would make resuling ref panel file leaner and faster to obtain over the internet.

  # Create unique filename based on input parameters. If already exists, don't need to recompute.
  ref_gds_file <- paste0(misc_config$scratch_dir,"/",digest::digest(list(misc_config$ref_panel,regions_bed_format,chr_nm_map)),".gds")
  if(!file.exists(ref_gds_file)) {

    if(tools::file_ext(misc_config$ref_panel[1]) == "gds") {
      # Make a copy s.t. the user's original GDS file will not be modified.
        # Also slim the copy down to only the required regions.
      tmp <- seqOpen(misc_config$ref_panel)
      seqSetFilterChrom(tmp, regions_bed_format$chr, # TODO: use chr_nm_map, to deal w/ chr naming styles wackier than just "chr" prefix (handled by default ignore.chr.prefix="chr"). And then rename the chrs in the copy.
                   from.bp = regions_bed_format$pos_min,
                     to.bp = regions_bed_format$pos_max)
      seqExport(ref, ref_gds_file, info.var="", fmt.var="")
      seqClose(misc_config$ref_panel); rm(tmp)

    } else { # Take given VCF(s) and convert to GDS
      source("inc/vcfs2Gds.R")
      vcfs2Gds(files = misc_config$ref_panel, 
               output_name = ref_gds_file,
               regions = regions_bed_format,
               exclude_annos = c("FORMAT","INFO"),
               chr_nm_map = chr_nm_map,
               scratch_dir = misc_config$scratch_dir)
    }
  }
  ref <- seqOpen(ref_gds_file, readonly=F)
}


{ if(all(misc_config$ref_panel == "my_ld")) refpanel_ids <- NULL # TODO: Reduce(union, etc.. Maybe Reduce(union, lapply(ld_config$filepath, fread, nrow=1)))
  else                                      ref_panel_ids <- seqGetData(ref,"annotation/id") }

# TODO:TODO: TEMPORARY space for my_ld ID stuff
ld <- read.table("../GCKR_AS", header=T, row.names=1, sep=' ', check.names=F)
identical(rownames(ld), colnames(ld))
setDT(ld)
setattr(ld,"row.names",NULL)
fwrite(ld, "../GCKR_AS_no_rn", sep=' ')


# 1. reformat this LD to plain matrix... w/ colnames (MANUAL, not in this script's code)
  # Do fwrite(setDT(read.table(<FILE>, headet=T, row.names=1, sep=' ', check.names=F)))
  # OR just cut or awk or s/t. Nah can't get it to work
# 2. read the ids from ecah file (tried readLines, but'd have to strsplit)
ids <- names(fread("../GCKR_AS",nrow=0))
# TODO:TODO: TEMPORARY



# If ref panel has no rsids,get some
if(!any(grepl("^rs", ref_panel_ids))) { # TODO: Actually can skip this if ref panel and ALL sumstats are of the same build. Can just use ch:pos:allele for e/t then.

  # Much faster to download the dbSNP file and access it locally. TODO: maybe if ~<500 variants, just use URL
  dbsnp_vcf <- {
    if      (misc_config$ref_panel_ref_genome == "hg19") "GCF_000001405.25.gz"
    else if (misc_config$ref_panel_ref_genome == "hg38") "GCF_000001405.40.gz"
    else stop("TODO")
  }
  if(!file.exists(dbsnp_vcf)) {
    message("Downloading dbSNP file locally for faster access.") # TODO: better message
    download.file(url = paste0("https://ftp.ncbi.nih.gov/snp/latest_release/VCF/",dbsnp_vcf)) # TODO: put in better place than just working dir
    download.file(url = paste0("https://ftp.ncbi.nih.gov/snp/latest_release/VCF/",dbsnp_vcf,".tbi")) # TODO: put in better place than just working dir
  }

  source("inc/bcftoolsPipableCmd.R")
  bcftools_cmd <- bcftoolsPipableCmd(
    files = dbsnp_vcf,
    regions = data.table(seqGetData(ref,"chromosome"), seqGetData(ref,"position")-1, seqGetData(ref,"position")),
    query = "%ID\t%CHROM\t%POS\t%REF\t%ALT\n",
    chr_nm_map = chr_nm_map,
    scratch_dir = misc_config$scratch_dir)

  dbsnp_var_id_info <- fread(cmd = bcftools_cmd, col.names=c("rs_id", "chr", "pos", "ref", "alt"))
  ref_var_id_info <- data.table(chr=seqGetData(ref,"chromosome"), pos=seqGetData(ref,"position"), ref=seqGetData(ref,"$ref"), alt=seqGetData(ref,"$alt")) # TODO: for my_ld, make this same thing but from parsed chr:pos IDs

  tmp <- dbsnp_var_id_info[
           ref_var_id_info,
           on=.(chr,pos)
         ][
           mapply(i.ref,i.alt,ref,alt, FUN = function(ir,ia,r,a)
             all( c(ir,ia) %in% c(r,strsplit(a,',')[[1]]) ))
         ][
           ref_var_id_info,
           on=c("chr","pos",i.ref="ref",i.alt="alt"),
           roll=T,
           mult="first"
         ]
  identical(tmp$chr, ref_var_id_info$chr)
  identical(tmp$pos, ref_var_id_info$pos)
  identical(tmp$i.ref, ref_var_id_info$ref)
  identical(tmp$i.alt, ref_var_id_info$alt)
  # TODO: Seems OK, but just quddruple-check that this is actaully correct b/c I'm not 100% confident w/ join operations yet.
    # sum(is.na(tmp$rs_id) | duplicated(tmp$rs_id)) / nrow(tmp) is ~5% which seems a bit high.
    # Then again this could just be b/c multiallelics are decomposed in the ref panel but not in dbSNP?

  seqAddValue(ref, "annotation/id", tmp$rs_id, replace=T)

  # Filter variants w/ duplicate rsID (indicative of variants in the reference panel which have been merged in newer versions of dbSNP)
  # Filter variants w/  missing  rsID (                                                             withdrawn                         )
  # TODO: use rsID synonyms and stuff to try to keep as many variants as possible?
  seqSetFilter(ref, variant.sel = !duplicated(tmp$rs_id) & !is.na(tmp$rs_id))

  rm(tmp, ref_var_id_info, dbsnp_var_id_info)
}
```

## Reference panel `sample_info`
```{r}
if(any(misc_config$ref_panel != "my_ld")) {
  sample_info <- fread(misc_config$sample_info_file)[, c(misc_config$sample_id_field, misc_config$sample_ancestry_field), with=F]
  setnames(sample_info, c("id","ancestry"))
  sample_info$ancestry <- factor(toupper(sample_info$ancestry))
}
```


# Filter sumstats using ref panel
Only keep variants which:
1. Are within the loci specified in `config.loci`
2. Have ancestry-specific MAF above `r misc_config$maf_threshold` for that sumstat file's ancestry
3. Are present in both sumstats and reference panel
```{r}
#| output: false
loci <- loci_configs[[misc_config$ref_panel_ref_genome]]

seqSetFilterChrom(ref, as.character(loci$chr), loci$pos_min, loci$pos_max, is.num=F, intersect=T)

samples_of_each_ancestry <- tapply(sample_info$id, INDEX=sample_info$ancestry, FUN = identity) 
ancestries_maf_mac_miss <- lapply(samples_of_each_ancestry, function(samples_of_an_ancestry) {
    seqSetFilter(ref, sample.id = samples_of_an_ancestry)
    seqGetAF_AC_Missing(ref, minor=T)
})
seqResetFilter(ref, variant=F)

invisible(sapply(data_config$ancestry, function(anc) if(anc %ni% names(ancestries_maf_mac_miss)) { print(paste("ERROR: Ancestry",anc,"mentioned in config.data is not an ancestry in the reference panel:", paste(collapse=", ", names(ancestries_MAFs)), "\nPlease remove this dataset from config.data, or if this was just a typo, change the ancestry name to match one of those in the reference panel.")); stop() })) # TODO: this belongs earlier, when loading in the data. But a nice sanity check to have for now.

sumstats <- mapply(sumstats, data_config$ancestry, SIMPLIFY=F,
  FUN = function(s, anc) {
    ancestry_maf_mac_miss <- ancestries_maf_mac_miss[[anc]]

    # TODO: also PASS the FILTER?
    # seqGetData(ref, "annotation/filter")=="PASS"
    

    my_filter <- rep(T, sum(seqGetFilter(ref)$variant.sel))
    # TODO: multiallelic filter using seqNumAllele()?
    if(!anyNA(filt(ref)))
      my_filter <- my_filter & filt(ref) == "PASS" # TODO: is bcftools annotate -x 'FORMAT,INFO' removing FILTER?
    if(!is.null(misc_config$maf_threshold))
      my_filter <- my_filter & ancestry_maf_mac_miss$af > misc_config$maf_threshold
    if(!is.null(misc_config$mac_threshold))
      my_filter <- my_filter & ancestry_maf_mac_miss$ac > misc_config$mac_threshold
    if(!is.null(misc_config$missingness_threshold))
      my_filter <- my_filter & ancestry_maf_mac_miss$miss > misc_config$missingness_threshold

    seqSetFilter(ref, action = "push+intersect", variant.sel = my_filter)

    s <- s[rs_id %in% seqGetData(ref,"annotation/id")]
    seqFilterPop(ref)
    s
})
#save(sumstats, samples_of_each_ancestry, file="sumstats-filtered.RData")
```


# Annotation
## Add Annotation Columns
```{r}
# TODO: PAINTOR's script is fast, but would be more elegant to do in R than system calls to Python
anno_colss <- mapply(sumstats, 1:length(sumstats), SIMPLIFY=F, FUN = function(sumstat,i) {
  scratch <- misc_config$scratch_dir
  py_env_dir <- paste0(scratch,"/py_env/")

  fwrite(sumstat[, .(chr,pos)], paste0(scratch,"/sumstat",i), sep=' ')
  writeLines(anno_config$bed_file, paste0(scratch,"/config.anno.tmp"))

  if(!dir.exists(py_env_dir)) {
    system(paste0("virtualenv ",py_env_dir,"; ",
                  "source ",py_env_dir,"bin/activate; ",
                  "python -m pip install numpy"))
  }
  system(paste0("source ",py_env_dir,"bin/activate; ",
                "python3 third_party/PAINTOR_V3.0/PAINTOR_Utilities/AnnotateLocus.py",
                  " -l ",scratch,"/sumstat",i," -c chr -p pos",
                  " -i ",scratch,"/config.anno.tmp",
                  " -o ",scratch,"/anno_cols.txt"))

  anno_cols <- fread(paste0(scratch,"/anno_cols.txt"))
  names(anno_cols) <- anno_config$name

  unlink("config.anno.tmp")
  unlink(paste0(scratch,"/anno_cols.txt"))
  unlink(paste0(scratch,"/sumstat",i))
  anno_cols 
})

# Rm cols that are all-0 in all ancestries TODO: or all < some threshold
n_anno_hits <- as.data.frame(lapply(anno_colss,sapply,sum))
anno_no_hits_in_all_datasets <- rownames(n_anno_hits)[rowSums(n_anno_hits)==0]
lapply(anno_colss, function(anno_cols) anno_cols[, (anno_no_hits_in_all_datasets) := NULL])
message("The following annotations were removed because no variants in any dataset were positive for them:\n", paste(collapse='\n', anno_no_hits_in_all_datasets))
# NOTE: if everything is 0, might be because the contig names in the bed files don't match those of chr_nm_map (which is what sumstats should now be onverted to)

sumstats <- mapply(sumstats, anno_colss, SIMPLIFY=F, FUN=cbind)
```

## fgwas
```{r}
# TODO: Move most of this fgwas stuff to a separate function
invisible(lapply(sumstats, function(sumstat) sumstat[,SEGNUMBER := as.integer(locus)]))

if(!dir.exists("in/fgwas")) dir.create("in/fgwas", recursive=T)
if(!dir.exists("out/fgwas")) dir.create("out/fgwas")

# Functions here assume sumstat_file is formatted properly for fgwas -fine. I.e.:
  # has "SNPID", "CHR", "POS", "Z", "F", "N", "SE", "SEGNUMBER", and anno cols.
    # CHR must named "chr#" not just "#".
    # F and N can be blank as they are overridden by SE, but must be present nonetheless for some reason.
    # Sorted by SEGNUMBER,CHR,POS

# Iteratively adds the anno which most increases the model's likelihood, until no more improvement. 
fgwasGetBestLikelihoodAnnos <- function(sumstat_file, annos_to_try, annos_acc, best_llk) {

  llks <- sapply(annos_to_try, function(anno) { # TODO: embarassingly parallel lapply
    fgwas_out_prefix <- paste0("out/fgwas/",anno_to_try,"-",sumstat_file)

    model <- paste(collapse='+',c(annos_acc, anno_to_try));    if(length(annos_acc)==0) model <- anno_to_try
    system(paste0("../third_party/fgwas/src/fgwas -fine",
                  " -i ", sumstat_file,
                  " -w ", model, 
                  " -o ", fgwas_out_prefix))

    llk <- fread(paste0(fgwas_out_prefix,".llk"))[1,2]
  })
 
  if(max(llks) > best_llk) {
    new_best_llk <- max(llks)
    anno_to_add  <- names(llks)[which.max(llks)]
    new_anno_acc <- c(anno_acc, anno_to_add)

    return(fgwasGetBestLikelihoodAnnos(sumstat_filename, annos_to_try[!anno_to_add], new_anno_acc, new_best_llk) )
  } else return(anno_acc)
}
# The above function is recursive.
# tailr::loop_transform makes it slightly faster and makes sure it doesn't overflow the stack, because R doesn't implement tail recursion.
  # (Tail recursion is when you don't need to hold all recursive calls in memory because your recursive function returns only either a call to itself or a final value.)
fgwasGetBestLikelihoodAnnos <- tailr::loop_transform(fgwasGetBestLikelihoodAnnos)

# Tests which penalty value has the best cross-validation likelihood, given the model fgwasGetBestLikelihoodModel()
fgwasGetBestXValidationPenalty <- function(sumstat_file, annos) {
  # TODO: Does f(model,penalty) have only one minimum? If so, could optimize by making more intelligent steps than just 0.05 intervals.
  # TODO: Go finer than steps of 0.05?
  penalties <- seq(0, 1, 0.05)

  xv_llks <- lapply(penalties, function(penalty) { # TODO: embarassingly parallel lapply
    fgwas_out_prefix <- paste0("out/fgwas/",as.character(penalty),"-",sumstat_file)

    model <- paste(collapse='+', annos)
    system(paste0( "../third_party/fgwas/src/fgwas -fine -xv -print",
                   " -i ", sumstat_file,
                   " -w ", model, 
                   " -p ", penalty,
                   " -o ", fgwas_out_prefix))

    a <- fread(paste0(fgwas_out_prefix,".ridgeparams"))
    xv_llk <- a[,nrow(a)] # Kinda wack, but fgwas puts the x-validation likelihood result in the last line of the .ridgeparams output files.
  })

  return(list(
    best_xv_llk = max(xv_llks),
    best_penalty = penalties[which.max(xv_llks)]
  ))
}

# Iteratively drops annos from the model, until the cross-validation likelihood is maximized.
fgwasGetBestXValidatedAnnos <- function(sumstat_file, xv_penalty, annos, best_xv_llk) {
  # TODO: Here I assume that dropping an anno in the middle of the model *might* cause an anno earlier in the model to now also become worth it to drop the llk if dropped, even if that wasn't the case before dropping the middle anno.
    # This might be overly cautious, in which case, this could be optimized by only needing to traverse the model once, dropping whatever increases the llk without having to look back.
  xv_llks <- sapply(annos, function(anno_to_drop) { # TODO: embarassingly parallel lapply
    fgwas_out_prefix <- paste0("out/fgwas/drop_",anno_to_drop,"-",sumstat_file)

    model <- paste(collapse='+', annos[!anno_to_drop])
    system(paste0( "../third_party/fgwas/src/fgwas -fine -xv -print",
                   " -i ", sumstat_file,
                   " -w ", model,
                   " -p ", penalty,
                   " -o ", fgwas_out_prefix))

    a <- fread(paste0(fgwas_out_prefix,".ridgeparams"))
    xv_llk <- a[,nrow(a)]
  })

  best_new_xv_llk <- max(xv_llks)
  if(max(xv_llks) > best_xv_llk) {
    new_best_xv_llk <- max(xv_llks)
    anno_to_drop <- names(xv_llks)[which.max(xv_llks)]
    new_annos <- annos[!anno_to_drop]

    return(fgwasGetBestXValidatedAnnos(sumstat_file, xv_penalty, new_annos, new_best_xv_llk))
  } else return(annos)

}
fgwasGetBestXValidatedAnnos <- tailr::loop_transform(fgwasGetBestXValidatedAnnos)


# Takes the sumstats, formats them in a way fgwas likes, then applies the above functions to get the annotations of the best-likelihood model.
best_annos <- 
lapply(sumstats, function(s) { # TODO: embarassingly parallel lapply
  s <- s[, .SD, .SDcols=c( "rsid","chr","pos","z","se","SEGNUMBER",anno_config$name)]
  setnames(s,           c("SNPID","CHR","POS","Z","SE","SEGNUMBER",anno_config$name))
                                   # fgwas requires specific header names.
  s <- s[order(SEGNUMBER,CHR,POS)] # fgwas requires ordering by SEGNUMBER (if -fine option), and chr and pos.
  s[,chr := paste0("chr",CHR)]     # fgwas uses hg19 "chr#" syntax, not just the number.
  s[,`:=`(F=0, N=0)]               # fgwas errors if missing F or N col, even though overridden by SE.

  input_filename <- paste0("in/fgwas/",s_nm,"-fgwas.gz")
  fwrite(s, input_filename, sep=' ', compress="gzip")

  # Old, to test just one anno at a time, use me instead of the below code. # TODO: rm me eventually
  #lapply(anno_config$name, function(anno) {
  #  system(paste0( "../third_party/fgwas/src/fgwas -i ", input_filename, " -fine",
  #                 " -w ", anno,
  #                 " -o out/fgwas/",s_nm,"-",anno ))
  #})

  annos   <- fgwasGetBestLikelihoodAnnos(input_filename, anno_config$name, c(), 0)
  l       <- fgwasGetBestXValidationPenalty(input_filename, annos)
  return(    fgwasGetBestXValidatedAnnos(input_filename, annos, l$best_penalty, l$best_xv_llk))
})
```

## Subset
Split the summary stats into sets of subsets, one per locus.
```{r}
sumstats <- lapply(sumstats, '[', order(rsid)) # TODO: prefer chr:pos:allele IDs
subset_sumstatss <- lapply(1:nrow(loci_config), function(r) {
                    mapply(sumstats, data_config$ref_genome, SIMPLIFY=F, FUN = function(s, ref_genome) {
  s[ chr == loci_configs[[ref_genome]][r,chr]     &
     pos >= loci_configs[[ref_genome]][r,pos_min] &
     pos <= loci_configs[[ref_genome]][r,pos_max] ]
})})
names(subset_sumstatss) <- loci_config$locus
```

# LD
Calculate LD per ancestry per locus.
```{r}
# Signed Pearson r.
# Returned LD matrix's rows/cols are sorted to match the order of the given rs_ids.
# Filters on the GDS file ~are~ taken into account. The GDS file will be returned with the same filters as before.
ldCalc <- function(gds_file, rs_ids, sample_ids=NULL, ref_alleles=NULL, outfile_name=NULL, n_threads=parallel::detectCores()) {
  if(is.null(ref_alleles)) warning("calc_ld() warning: specifying your data's ref_alleles is highly recommended! If the reference alleles (i.e. non-effect alleles) in your data =/= those in the reference panel, the LD will be incorrect.")
  seqFilterPush(gds_file) # Save user's prexisting filter on the file if they have one

  seqSetFilterAnnotID(gds_file, rs_ids)
  if(!is.null(sample_ids)) seqSetFilter(gds_file, sample.id = sample_ids, action="intersect")

  tmp_filename <- "/tmp/snpgds_format_file.gds"
  seqGDS2SNP(gds_file, tmp_filename, compress.geno="", compress.annotation="")
  tmp <- snpgdsOpen(tmp_filename, readonly=F)
  #print(paste(identical(read.gdsn(index.gdsn(tmp,"snp.rs.id")), seqGetData(gds_file,"annotation/id")))) # TRUE

  # Sort user-given ref_alleles to match the order of the GDS file before allele switching!
  # Why not sort the GDS file to match user input instead? B/c raw data files should not be edited.
  ref_alleles <- ref_alleles[match(seqGetData(gds_file,"annotation/id"), rs_ids)]
  snpgdsAlleleSwitch(tmp, toupper(ref_alleles))
  ld <- snpgdsLDMat(tmp, slide=0, method="corr", num.thread=n_threads)$LD

  snpgdsClose(tmp) # Don't need SNP GDS format file anymore
  unlink(tmp_filename)

  # Sort LD matrix to match the order of the user-given rs_ids.
  rs_ids_order <- match(rs_ids, seqGetData(gds_file,"annotation/id"))
  ld <- ld[rs_ids_order, rs_ids_order]

  if(!is.null(outfile_name)) {
    if(!dir.exists(dirname(outfile_name))) dir.create(dirname(outfile_name), recursive=T)
    fwrite(ld, outfile_name, sep=' ', col.names=F, compress="gzip")
  }

  seqFilterPop(gds_file) # Set filter to however it was before
  ld
}
# Note: it doesn't (and shouldn't!) matter that the seqSetFilterChrom filter from before is still active.

ld_rsid_orders <-
  sapply(       loci_config$locus,     simplify=F, function(l) {
  sapply(unique(data_config$ancestry), simplify=F, function(anc) {
    samples_of_an_ancestry <- samples_of_each_ancestry[[anc]]
    sumstats_of_an_ancestry <- subset_sumstatss[[l]][which(data_config$ancestry == anc)]
    common_rsids_in_sumstats_of_an_ancestry <- Reduce(union, lapply(sumstats_of_an_ancestry, '[', j=rsid))

    # TODO: not generalizable if multiple datasets in an ancestry with different rsids and/or different effect alleles.
      # Simple solution would be in data pre-proc: just flip alleles & β & z & etc..
      # For now w/ just the DIAMANTE data w/ one dataset per ancestry it's fine. Even the different ancestries have matching a0/a1, I checked.
    a_sumstat_of_an_ancestry <- sumstats_of_an_ancestry[[1]][rsid %in% common_rsids_in_sumstats_of_an_ancestry]
    #print(paste(identical(a_sumstat_of_an_ancestry$rsid, common_rsids_in_sumstats_of_an_ancestry)))
    ref_alleles <- a_sumstat_of_an_ancestry$a0

    ldCalc(ref, rs_ids = common_rsids_in_sumstats_of_an_ancestry,
             sample_ids = samples_of_an_ancestry,
            ref_alleles = ref_alleles,
           outfile_name = paste0("in/ld/",l,"-",anc,".ld.gz"),
              n_threads = 6L) 
  
    common_rsids_in_sumstats_of_an_ancestry # Return the order of the LD mat's rows/cols, important!
})})

save(ld_rsid_orders, file="in/ld/ld_rsid_orders.RData") # TODO: Save as separate files, with one col of just rsids? This way, LD could be used by non-R programs more easily.
```


# TODO TODO
# Do allele switching so that sumstats and LD match
```{r}

```


# Run Fine-Mapping Methods
## SuSiEx
### Prep Input
SuSiEx is a command-line python program. It expects the following:\
A summary stats file, as follows:\ 
TODO: make nice table\
The β column _must_ be named "BETA" or "OR", but besides that they may be named arbitrarily.
```{r}
if(!dir.exists("in/susiex")) dir.create("in/susiex")
# TODO: ensure data contains the required columns
lapply(loci_config$locus, function(l) {
mapply(subset_sumstatss[[l]], names(sumstats), FUN=function(s, nm) {
  s <- s[, .(chr,pos,rsid,a0,a1,b,se,p)]
  setnames(s, old="b", new="BETA", skip_absent=T)
  fwrite(s, paste0("in/susiex/",l,"-",nm), sep='\t')
})})
```

```{r}
# TODO: PLINK stuff :(
  # NOTE TO SELF: see daily note 230605. Letting SuSiEx recompute its own LD for now just to get things working.
```

### Run SuSiEx
```{r}
if(!dir.exists("out/susiex")) dir.create("out/susiex", recursive=T)
err <- lapply(1:nrow(loci_config), function(r) {
  system(paste("../third_party/SuSiEx/bin/SuSiEx",
               "--sst_file", paste(collapse=',', paste0("in/susiex/",loci_config[r,locus],"-",names(sumstats))),
               "--n_gwas",   paste(collapse=',', data_config$n),
               "--ld_file",  paste(collapse=',', paste0("in/ld/",loci_config[r,locus],"-",unique(data_config$ancestry),"-susiex")), # TODO: compute own LD
               "--ref_file ../data/ref/1kg/plink_format/eas/g1000_eas,../data/ref/1kg/plink_format/eur/g1000_eur,../data/ref/1kg/plink_format/sas/g1000_sas", # TODO:
               "--plink plink", # TODO:
               "--out_dir out/susiex",
               "--out_name", loci_config[r,locus],
               #"--level 0.95", # Default
               "--chr ", loci_config[r,chr],
               "--bp ", paste(sep=',', loci_config[r,pos_min], loci_config[r,pos_max]),
               "--chr_col", paste(collapse=',', rep(1,nrow(data_config))),
               "--bp_col",  paste(collapse=',', rep(2,nrow(data_config))),
               "--snp_col", paste(collapse=',', rep(3,nrow(data_config))),
               "--a1_col",  paste(collapse=',', rep(4,nrow(data_config))),
               "--a2_col",  paste(collapse=',', rep(5,nrow(data_config))),
               "--eff_col", paste(collapse=',', rep(6,nrow(data_config))),
               "--se_col",  paste(collapse=',', rep(7,nrow(data_config))),
               "--pval_col",paste(collapse=',', rep(8,nrow(data_config))),
               "--maf", misc_config$maf_threshold, # TODO: 0 is not allowed, why?
               #"--min_purity 0.5", # Default. Even if some credsets are impure and not practically useful, still want all the posteriors for comparison purposes. TODO: Purity filter of 0 causes errors for some loci, see daily note 230621. 0.01 is totally low enough but it not being 0 still bothering me. Low priorty to look into.
               "--mult-step True",
               "--keep-ambig True",
               "--threads", 6L # TODO: make more general, or allow input
               # defaults: --max_iter=100, --pval_thresh=1e-5, --tol=1e-4, --n_sig=5
  ))
})
# TODO: use "err" to print a warning if SuSiEx fails for some loci, and specify the loci.
```

## PAINTOR
### Prep Input
PAINTOR only works on variants shared by ALL input datasets.\
TODO: desc
```{r}
if(!dir.exists("in/paintor")) dir.create("in/paintor")
# input.files
writeLines(loci_config$locus, "in/paintor/input.files")

# Locus files. Format:
# rsid  pop1_z  pop2_z ...
#  rs1   -3.14  -1.337
#  ... <no NAs allowed>
desired_rsidss <- sapply(loci_config$locus, simplify=F, function(l) {
  signif_rsids <- Reduce(union,     lapply(subset_sumstatss[[l]], '[', j=rsid)) # NOTE: optional hardcoded p threshold here
  common_rsids <- Reduce(intersect, lapply(subset_sumstatss[[l]], '[',           j=rsid))
  desired_rsids <- intersect(signif_rsids, common_rsids)
})

lapply(loci_config$locus, function(l) {
  desired_rsids <- desired_rsidss[[l]]
  desired_sumstats <- lapply(subset_sumstatss[[l]], function(s) s[rsid %in% desired_rsids]) # We sorted before, so all sumstats' rsids are in the same order even after subsetting. 

  paintor_locus_file <- Reduce(cbind, lapply(desired_sumstats, '[', j=z), desired_rsids) # Can add more metadata than just rsid if you want
  colnames(paintor_locus_file) <- c("rsid", paste0(names(sumstats),"_z"))
  fwrite(paintor_locus_file, paste0("in/paintor/",l), sep=' ')
})

# LD files. Format: Pearson r, space-delim'd, no header, Must have same number of rows and order as locus file.
lapply(loci_config$locus, function(l) {
  desired_rsids <- desired_rsidss[[l]]
  ld_filenames <- paste0("in/ld/",l,"-",unique(data_config$ancestry),".ld.gz")
  desired_filenames <- paste0("in/paintor/",l,".ld_",unique(data_config$ancestry))
  lds <- lapply(ld_filenames, function(f) as.matrix(fread(f)))
  mapply(lds, ld_rsid_orders[[l]], desired_filenames, SIMPLIFY=F, FUN = function(ld, ld_rsid_order, nm) {
    # return(c(length(ld_rsid_order),length(desired_rsids), sum(ld_rsid_order %in% desired_rsids)))
    # TODO: This code could definitely be made clearer.
    desired_subset <- ld_rsid_order %in% desired_rsids # TODO: rename ld_rsid_order variable to not "order". Because it's just rsids.
    ld <- ld[desired_subset, desired_subset]
    ld_rsid_order <- ld_rsid_order[desired_subset]

    desired_order <- match(desired_rsids, ld_rsid_order)
    ld <- ld[desired_order,desired_order]

    fwrite(ld, nm, sep=' ', col.names=F)
  })
}) # TODO: A more elegant way? Unfortunately PAINTOR insists having all its input files in a single dir, so that means either cluttering in/ld/, changing filesnames in in/ld/ to use weird naming conventions, or making copies.... And PAINTOR doesn't accept compression so copies it is I guess....
# AND it wants to take in all loci at once, which means you have to store ALL the (decompressed!) LD files at once w/o being able to delete them as you go.

# Annotation files. Format:
# annot1  annot2 ...
# 0 or 1  0 or 1 
# Must have same number of rows and order as locus file.
# TODO: dummy all-0s file for now
lapply(loci_config$locus, function(l) {
  desired_rsids <- desired_rsidss[[l]]
  writeLines(c("dummy_annot", rep(0,length(desired_rsids))),
             paste0("in/paintor/",l,".annotations"))
})
```

### Run PAINTOR
```{r}
if(!dir.exists("out/paintor")) dir.create("out/paintor", recursive=T)
system(paste("../third_party/PAINTOR_V3.0/PAINTOR",
               "-input in/paintor/input.files",
               "-in in/paintor/",
               "-out out/paintor/",
               "-Zhead",  paste(collapse=',', paste0(names(sumstats),"_z")),
               "-LDname", paste(collapse=',', paste0("ld_",data_config$ancestry)),
               "-annotations dummy_annot",
               "-enumerate 1"
))
```
