---
title: `r Sys.Date()`
format: html
toc: true
---

TODO: stamp each render/run of this script with a UUID, and add log/<method>/<UUID> dirs
TODO: according to ?system, system2() is better, so switch to that eventually I guess. More portable. 
TODO: replace all the "r"s or "c"s with "i"s or "j"s in apply functions like mapply(1:nrow(x), 1:ncol(x), function(r,c) {...})

# Lib
```{r}
#| output: false
library(data.table)
library(SeqArray) # TODO: don't need this unless using certain ref panels, so load later.
library(SNPRelate)
```

# Config files
```{r}
misc_config <- list()
              source("config.misc")
data_config <- fread("config.data")
loci_config <- fread("config.loci")
anno_config <- fread("config.anno")
```
TODO: printout "stamp" of configs s.t. in the rendered doc, it is clear what was run.

## Validate input
```{r}
"%ni%" <- Negate("%in%")
# TODO: Why? Because the rest of the code is cleaner under the assumption all input datasets are valid, and error messages early save the user time.
  # For each row in data_config, check which columns are specified for each dataset to make sure everything that's required by all misc_config$fm_methods is there.
  # Make sure every dataset's ancestry is a valid ancestry in the reference panel too
  # Use fread(nrows=3) or s/t to take a peek and check that the data type of each column is reasonable. TODO: move this down into a separate input data validation step. Or trust the user some more and just use colClasses in data reading (although would hurt if failure on loading last dataset)
  # Verify valid reference genome names
if(misc_config$ref_panel %ni% c("1kG", "TOPLD", "gnomAD")) stop(paste0("Invalid reference panel: ", misc_config$ref_panel, ". Please correct this in config.misc."))
if(!dir.exists(misc_config$scratch_dir)) stop(paste0("Invalid scratch directory:", misc_config$scratch_dir, ". Please correct this in config.misc."))
```

## `liftOver` config files
More efficient to `liftOver` `config.loci`/`.anno` to the sumstats' reference genomes rather than vice-versa. The summary stats are likely big files, especially now, before we have had the chance to subset them to only the necessary variants.\
In addition, it is not recommended (source [1](http://genome.ucsc.edu/FAQ/FAQreleases.html#snpConversion), [2](https://groups.google.com/a/soe.ucsc.edu/g/genome/c/wkgf5DXpwEc/m/wDpsjyDVCQAJ)) to `liftOver` individual variants by their position—converting position via rsID is more accurate.\
That said, even rsIDs can be ambiguous ([source](https://gnomad.broadinstitute.org/help/why-is-this-variant-linked-to-the-wrong-dbsnp-rsid)), so it's better to avoid `lift`ing`Over` the sumstats if possible (although might have to later, to match the sumstats with a reference panel to get LD).\
TODO add blurb about how b37 hg19 CRCh37 are mostly equiv except mitochondria and "chr#" vs "#" and link sources, and how GRCh38 and hg38 are equiv AFAIK
```{r}
if(misc_config$ref_panel == "1kG"   ) misc_config$ref_panel_ref_genome <- "hg19"
if(misc_config$ref_panel == "TOPLD" ) misc_config$ref_panel_ref_genome <- "hg38"
if(misc_config$ref_panel == "gnomAD") misc_config$ref_panel_ref_genome <- "hg38"
```

### Convert to UCSC naming scheme
`liftOver` uses the UCSC naming scheme, so we need to too.
```{r}
#| output: false
unifyRefGenomeNaming <- function(config_file) {
  config_file[ref_genome %in% c("GRCh37", "b37", "hg19"), ref_genome := "hg19"]
  config_file[ref_genome %in% c("GRCh38",        "hg38"), ref_genome := "hg38"]
  config_file[, ref_genome := factor(ref_genome)]
} 
lapply(list(data_config, loci_config, anno_config), unifyRefGenomeNaming)

ucscChrNaming <- function(chr) { ifelse(is.integer(chr), paste0("chr",chr), chr) }
ucscChrNaming <- Vectorize(ucscChrNaming)
loci_config[, chr := ucscChrNaming(chr)]
```

### `liftOver`
```{r}
liftOver <- function(liftover_bin, chrs, pos_mins, pos_maxs, from_ref_genome, to_ref_genome, bed_file=NULL, out_file=NULL) {
  chain_file_path <- dirname(liftover_bin)
  chain_file_name <- paste0(from_ref_genome, "To", tools::toTitleCase(as.character(to_ref_genome)), ".over.chain.gz")
  chain_file <- paste0(chain_file_path,"/",chain_file_name)

  if(from_ref_genome == to_ref_genome) stop("Using liftOver to convert from one reference genome to the same reference genome!")
  if(!file.exists(paste0(chain_file))) stop(paste0("Please download the chain file https://hgdownload.soe.ucsc.edu/goldenPath/",from_ref_genome,"/liftOver/",chain_file_name," and add it to ",chain_file_path,"/."))
  # TODO: downloading automatically for the user would be nice, but Compute Canada nodes don't have internet. Maybe move this chain-file-getting to a bash script that happens during init? But then the init scripts would be coupled to the config files changing.

  if(is.null(bed_file)) {
    bed_file <- "to_lift.bed"
    fwrite(data.table(chrs,pos_mins,pos_maxs), bed_file, sep=' ', col.names=F)
  }
  if(is.null(out_file)) {
    out_file <- "lifted.bed"
  }

  system(paste(liftover_bin, bed_file, chain_file, out_file, "unmapped.tmp"))

  n_bed_rows <- as.integer(system(paste("wc -l", bed_file, "| awk '{print $1}'"), intern=T))
  n_unmapped <- as.integer(system(paste("wc -l unmapped.tmp | awk '{print $1}'"), intern=T))
  if(n_unmapped > 0) message(paste(n_unmapped,"/",n_bed_rows, "regions could not be liftOver'd."))

  out <- fread(out_file)
  unlink(c("to_lift.bed", "lifted.bed", "unmapped.tmp"))
  out
}
if(!dir.exists("out/liftover")) dir.create("out/liftover")

# Make separate versions of config.loci, lifted over to the necessary reference genome(s).
loci_configs <-
lapply(union(data_config$ref_genome, misc_config$ref_panel_ref_genome), function(dg) {
  lifted_loci_config <- copy(loci_config)
  lapply(unique(loci_config$ref_genome), function(lg) {
    if(dg != lg) {
      loci_of_a_ref_genome <- loci_config[ref_genome == lg]

      lifted <- liftOver("../third_party/liftover/liftOver",
                         loci_of_a_ref_genome$chr,
                         loci_of_a_ref_genome$pos_min,
                         loci_of_a_ref_genome$pos_max,
                         from_ref_genome = lg,
                           to_ref_genome = dg)

      i <- which(lifted_loci_config$ref_genome == lg)
      set(lifted_loci_config, i, "chr",     lifted[,1])
      set(lifted_loci_config, i, "pos_min", lifted[,2])
      set(lifted_loci_config, i, "pos_max", lifted[,3])
      set(lifted_loci_config, i, "ref_genome", dg)
    }
  })
  lifted_loci_config[, chr := sub("chr","",chr)] # Undo UCSC chr naming # TODO: make factor, and make a convenience chrFactor() function to use here and when reading sumstats?
  fwrite(lifted_loci_config, paste0("out/liftover/config.loci.",dg), sep=' ')
  lifted_loci_config
})
names(loci_configs) <- union(data_config$ref_genome, misc_config$ref_panel_ref_genome)

anno_configs <- 
lapply(unique(data_config$ref_genome), function(dg) {
  lifted_anno_config <- copy(anno_config)
  sapply(1:nrow(anno_config), function(i) {
    ag <- anno_config[i,ref_genome]
    if(dg != ag) {
      lifted_bed_file_name <- paste0("out/liftover/",anno_config[i,name],"-lifted_to_",dg,".bed")
      liftOver("../third_party/liftover/liftOver",
               bed_file = anno_config[i,bed_file],
               out_file = lifted_bed_file_name, 
               from_ref_genome = ag,
                 to_ref_genome = dg)

      lifted_anno_config[i, `:=`(ref_genome = dg,
                                 bed_file = lifted_bed_file_name)]
    }
  })
  fwrite(lifted_anno_config, paste0("out/liftover/config.anno.",dg), sep=' ')
  lifted_anno_config
})
names(anno_configs) <- unique(data_config$ref_genome)

# NOTE: don't worry if loci_configs' and anno_configs' ref_genome factor values are different for the same level label. They are still considered equal as you'd hope even compared between different data.tables.
```

# Data
## GWAS summary stats
```{r}
#| output: false
sumstats <- lapply(1:nrow(data_config), function(r) {

  loci <- loci_configs[[ data_config[r,ref_genome] ]] # Loci with coords lifted to the current data file's ref genome
  loci <- loci[, chr := as.integer(sub("chr","",chr))] # TODO: hardcoded assuming data has "#" not "chr#" format, but make it so that either works (hint: I'd like to use factors with chr# labels). Sorry future self I have o/ things to do rn. Also this modifies by reference, careful, remember to use copy().

  # AWK command given to fread(). Only loads the necessary variants. (Excludes those not in any config.loci locus, or with EAF of 0 or 1.)
  # If your data is huge but you only care to look at a few regions, this saves memory.
  chr_pos_eaf_preprocess_cmd <- paste0(
    "awk ",
    "-v chrs_string='",paste(collapse=' ', loci$chr),     "' ",
    "-v mins_string='",paste(collapse=' ', loci$pos_min), "' ",
    "-v maxs_string='",paste(collapse=' ', loci$pos_max), "' ",
    "'BEGIN {",
      "getline; print;", # Get header line
      # Turn the space-delimited strings into arrays
      "split(chrs_string, chrs, \" \");",
      "split(mins_string, mins, \" \");",
      "split(maxs_string, maxs, \" \");",
    "}",
    "{",
      "for(i=0; i<=",nrow(loci),"; i++) {",
        "if($",data_config[r,chr_col]," == chrs[i] && ",
           "$",data_config[r,pos_col]," >= mins[i] && ",
           "$",data_config[r,pos_col]," <= maxs[i]) { ",
          "print; break;", # Print locus rsid is in as the last col
        "}",
      "}",
    "}' ",
    data_config[r,filepath]
  )

  sumstat <- data_config[r, fread(cmd = chr_pos_eaf_preprocess_cmd,
                                   select=c(chr_col, pos_col, rsid_col, other_allele_col, effect_allele_col, eaf_col, b_col, se_col, z_col, p_col))]
  unspecified_cols <- is.na(data_config[r,.(chr_col, pos_col, rsid_col, other_allele_col, effect_allele_col, eaf_col, b_col, se_col, z_col, p_col)])
  setnames(sumstat,                      c("chr",   "pos",   "rsid",   "a0",             "a1",              "eaf",   "b",   "se",   "z",   "p"   )[!unspecified_cols])

  sumstat[,chr := factor(chr,levels=c("1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","X","Y","M"))] # TODO: make sure things will line up correctly even if user uses UCSC "chr#" naming or not.
  sumstat[,`:=`(a0=toupper(a0), a1=toupper(a1))]
  if("eaf" %in% names(sumstat)) sumstat <- sumstat[eaf > 0 & eaf < 1]

  # Infer some columns from the others if possible. For example, z-score can be calculated from β and SE.
  if(data_config[r, is.na(z_col) & !is.na(b_col) & !is.na(se_col)]) sumstat[, z := b/se]
  # TODO: Add N col input (per-snp N), and reverse-engineer N from p/z/se/af if n_col not given.

  sumstat
})
names(sumstats) <- basename(data_config$filepath)
```

## Reference panel
### (WIP) gnomAD
```{r}
all_rsids <- Reduce(union, lapply(sumstats, '[', j=rsid))
all_rsids_file <- paste0(misc_config$scratch_dir,"/all_rsids.txt")
writeLines(all_rsids, all_rsids_file)

lapply(1:nrow(loci_config), function(i) {
  loci <- loci_configs[[misc_config$ref_panel_ref_genome]]

  l_vcf <- paste0(misc_config$scratch_dir,"/",loci[i,locus],"_gnomad.vcf")
  l_gds <- paste0(misc_config$scratch_dir,"/",loci[i,locus],"_gnomad.gds")

  # TODO: only take the INFO fields I need
  loci[i, system(paste0( "bcftools view",
                         " -r chr",chr,":",pos_min,"-",pos_max,   # TODO: hardcoded again to use "chr#" which is what gnomAD uses; the problem is this code assumes that loci_config uses just "#" which I might change again later
                         " --include ID==@",all_rsids_file,
                         " gs://gcp-public-data--gnomad/release/3.1.2/vcf/genomes/gnomad.genomes.v3.1.2.sites.chr",chr,".vcf.bgz",
                         " > ", l_vcf ))]

  seqVCF2GDS(l_vcf, l_gds, storage.option="ZIP_RA", reference="hg38", digest=F) # See ?seqRecompress: VCF2GDS conversion takes tons of memory w/ default compression. Better to use lower compression during conversion, then use seqRecompress() after. No checksum digest b/c I'm a gangsta, totally not b/c I'm impatient
  seqRecompress(    l_gds,                "LZMA")
  unlink(l_vcf)
})

unlink(all_rsids_file)

# TODO: seqMerge? or maybe not?
```

### (WIP) TOPLD
```{r}
# AWK won't work here b/c files are big (10mil rows+) and I'm searching for matches for 10k's of rsids inside.
# I suspect the awk is failing b/c I have to pass all the rsids so the shell command is too long.
# Anyways, the efficient thing to do here is unfortunately load the whole file into memory first....
  # Not in an lapply loop though, but a for loop.
#ref_annos <- sapply(unique(data_config$ancestry), simplify=F, function(anc) {
#  filepath <- paste0("../data/ref/topmed/topld/anno/",anc,"_topld_anno.csv")
#  sumstats_of_ancestry <- sumstats[which(data_config$ancestry == anc)]
#  rsids_of_ancestry <- Reduce(union, lapply(sumstats_of_ancestry, '[', j=rsid))
#  if(anyDuplicated(rsids_of_ancestry)>0) {stop("wat")}
#
#  shared_rsid_preprocess_cmd <- paste0(
#    "awk ",
#    "-v rsids_string='",paste(collapse=' ', rsids_of_ancestry), "' ", 
#    "'BEGIN {",
#      "getline; print;", # Get header line
#      "n_rsid=split(rsids_string,rsids,\" \")", # Turn the space-delimited string of rsids into an array
#    "}",
#    "{", # Only keep rows of rsids shared w/ the sumstats. And only keep the Position ($1) and rsID ($2) columns.
#      "for(i=0; i<=n_rsid; i++) {",
#        "if($2==rsids[i]) {print $1,$2}",
#      "}",
#      #"if(index(rsids_string, $2) > 0) {print}",
#    "}' ",
#    filepath
#  )
#  #return(shared_rsid_preprocess_cmd)
#
#  fread(cmd = shared_rsid_preprocess_cmd,
#           select=c("Position","rsID"),
#        col.names=c("pos",     "rsid"))
#})

# This is the original way, just loading whole file and then subsetting after.
#ref_annos <- sapply(unique(data_config$ancestry)[[1]], simplify=F, function(anc) {
##sapply(unique(data_config$ancestry), simplify=F, function(anc) {
#  topld_anno_file <- paste0("../data/ref/topmed/topld/anno/",anc,"_topld_anno.csv")
#  topld_rsids <- fread(topld_anno_file, select=c("rsID"), col.names=c("rsid"))
#
#  sumstats_of_ancestry <- sumstats[which(data_config$ancestry == anc)]
#  rsids_of_ancestry <- Reduce(union, lapply(sumstats_of_ancestry, '[', j=rsid))
#})
#
## Filter sumstats by variants shared with reference of same ancestry
#sumstats2 <- mapply(sumstats, data_config$ancestry, SIMPLIFY=F, FUN = function(s, s_anc) {
#  s[rsid %in% ref_annos[[s_anc]]$rsid]
#})

# Old code, reminder how to use the topld_api exe and that it just takes a file of rsids
#writeLines(sumstats[[1]]$rsid[1:100], "tmptopld.txt")
# Took over 5 mins already w/ just 100 variants.
#system(paste("../third_party/topld_api/topld_api -inFile tmptopld.txt -outputInfo outputInf.txt"))
```

### 1kG
```{r}
#| output: false
# TODO: Hardcoded to use 1kG for now. Possibly support TOPMed later.
ref <- seqOpen("../data/ref/1kg/gds_format/1KG_ALL.autosomes.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.gds", readonly=F, allow.duplicate=T)

# TODO: Move to a pre-process script and perform on a COPY of the data and store s/where. Annoying, but the "data/" dir should really remain untouched. Should be able to look in there and take md5sums and be confident e/t in there is raw.
  # Or, maybe not since each section of the file can have its own checksum. But in that case, use seqDigest on the original file then seqCheck here to verify data integrity.
sample_info <- fread("../data/ref/1kg/sample_info/integrated_call_samples_v3.20130502.ALL.panel")
identical(seqGetData(ref,"sample.id"), sample_info$sample)
seqAddValue(ref, "sample.annotation/ancestry", sample_info$super_pop, replace=T)
seqAddValue(ref, "sample.annotation/gender", sample_info$gender, replace=T)
rm(sample_info)
```

# Prep
## Filter
First, filter out useless/unusable variants from the summary stats: \
1. If variant's ancestry-specific MAF is 0 for that sumstat file's ancestry
2. If variant is not within the bounds of any loci specified in `locus.config`
3. If variant not also present in reference panel
```{r}
#| output: false
seqSetFilterChrom(ref, loci_configs$hg19$chr,
               from.bp=loci_configs$hg19$pos_min,
                 to.bp=loci_configs$hg19$pos_max)

samples_of_each_ancestry <- tapply(seqGetData(ref,"sample.id"), INDEX=seqGetData(ref,"sample.annotation/ancestry"), FUN = identity) 
ancestry_MAFs <- lapply(samples_of_each_ancestry, function(samples_of_an_ancestry) {
    seqSetFilter(ref, sample.id = samples_of_an_ancestry)
    seqAlleleFreq(ref, minor=T)
}) 
seqResetFilter(ref, variant=F)

invisible(sapply(data_config$ancestry, function(anc) if(anc %ni% names(ancestry_MAFs)) { print(paste("ERROR: Ancestry",anc,"mentioned in config.data is not an ancestry in the reference panel:", paste(collapse=", ", names(ancestry_MAFs)), "\nPlease remove this dataset from config.data, or if this was just a typo, change the ancestry name to match one of those in the reference panel.")); stop() })) # TODO: this belongs earlier, when loading in the data. But a nice sanity check to have for now.

# For each sumstat, keep only variants with MAF > 0 for the sumstat's ancestry.
# Reference panel used to calculate MAF.
# As a byproduct, also filters sumstats by variants shared with the reference.
sumstats <- mapply(sumstats, data_config$ancestry, SIMPLIFY=F,
  FUN = function(s, anc) {
    ancestry_MAF <- ancestry_MAFs[[anc]]
    seqSetFilter(ref, action = "push+intersect", variant.sel = ancestry_MAF>0)
    s <- s[rsid %in% seqGetData(ref,"annotation/id")]
    #print(sapply(seqGetFilter(ref),sum))
    seqFilterPop(ref)
    #print(sapply(seqGetFilter(ref),sum))
    s
})
#save(sumstats, samples_of_each_ancestry, file="filtered_sumstats.RData")
```

## Annotation
### Add Annotation Columns
```{r}
# TODO: Just using PAINTOR's script b/c it's pretty fast, but would be more elegant to do this in R not system calls to Python
anno_colss <- mapply(sumstats, 1:length(sumstats), SIMPLIFY=F, FUN = function(sumstat,i) {
  scratch_dir <- misc_config$scratch_dir
  sumstat$chr <- paste0("chr",sumstat$chr)
  fwrite(sumstat[, .(chr,pos)], paste0(scratch_dir,"/sumstat",i), sep=' ')

  writeLines(anno_config$bed_file, paste0(scratch_dir,"/config.anno.tmp"))

  # TODO: put this virtualenv cruft in misc$scratch_dir at least, if you're not going to write s/t more elegant solution w/o PAINTOR's script 
  if(!dir.exists("py_env")) system("virtualenv py_env; source py_env/bin/activate; python -m pip install numpy")
  system(paste0("source py_env/bin/activate; python3 ../third_party/PAINTOR_V3.0/PAINTOR_Utilities/AnnotateLocus.py -o ",scratch_dir,"/anno_cols.txt -c chr -p pos -i ",scratch_dir,"/config.anno.tmp -l ",scratch_dir,"/sumstat",i))

  anno_cols <- fread(paste0(scratch_dir,"/anno_cols.txt"))
  names(anno_cols) <- anno_config$name

  unlink("config.anno.tmp")
  unlink(paste0(scratch_dir,"/anno_cols.txt"))
  unlink(paste0(scratch_dir,"/sumstat",i))
  anno_cols 
})
lapply(anno_colss,lapply,sum) # Make sure everything isn't just 0
sumstats <- mapply(sumstats, anno_colss, SIMPLIFY=F, FUN=cbind)
```

### Add SEGNUMBER (and locus) Column
```{r}
# TODO: Could put this in the AWK preprocessing when first reading the file? But ugly. And unnecessary if user doesn't want annos.
lapply(sumstats, function(s) {
  s[,locus := factor(levels=loci_config$locus)]
  lapply(1:nrow(loci_config), function(l) {
    s[ chr == loci_config$chr[l] &
       pos >= loci_config$pos_min[l] &
       pos <= loci_config$pos_max[l],
       `:=`(locus=l, SEGNUMBER=l)       ]
})})
```

### fgwas
```{r}
if(!dir.exists("in/fgwas")) dir.create("in/fgwas")
if(!dir.exists("out/fgwas")) dir.create("out/fgwas")

mapply(sumstats, names(sumstats), SIMPLIFY=F, FUN=function(s, s_nm) {
  s <- s[, .SD, .SDcols=c( "rsid","chr","pos","z","se","SEGNUMBER",anno_config$name)]
  setnames(s,           c("SNPID","CHR","POS","Z","SE","SEGNUMBER",anno_config$name))
                                   # fgwas requires specific header names.
  s <- s[order(SEGNUMBER,CHR,POS)] # fgwas requires ordering by SEGNUMBER (if -fine option), and chr and pos.
  s[,chr := paste0("chr",CHR)]     # fgwas uses hg19 "chr#" syntax, not just the number.
  s[,`:=`(F=0, N=0)]               # fgwas errors if missing F or N col, even though overridden by SE.

  input_filename <- paste0("in/fgwas/",s_nm,"-fgwas.gz")
  fwrite(s, input_filename, sep=' ', compress="gzip")

  lapply(anno_config$name, function(anno) {
    system(paste0( "../third_party/fgwas/src/fgwas -i ", input_filename, " -fine",
                   " -w ", anno,
                   " -o out/fgwas/",s_nm,"-",anno ))
  })
})

# Tail-recursive version
# Function assumes sumstat is formatted properly for fgwas -fine. I.e.:
  # has "SNPID", "CHR", "POS", "Z", "F", "N", "SE", "SEGNUMBER", and anno cols.
    # CHR must be hg19 format, i.e. "chr#" not just "#".
    # F and N can be blank as they are overridden by SE, but must be present nonetheless for some reason.
  # sorted by SEGNUMBER,CHR,POS
# P1 iteratively adds the anno which increases the model's likelihood most, until no more gain.
getBestFgwasModelP1_recursive <- function(sumstat_filename_fgwas_compatible, annos_to_try, anno_acc, best_llk) {
  lapply(annos_to_try, function(anno_to_try) {
    # TODO: parallelize this fgwas call either w/ GNU parallel here or R parallel on outer fn call (including input file prep part)
    system(paste0( "../third_party/fgwas/src/fgwas -i ", sumstat_filename_fgwas_compatible, " -fine",
                   " -o out/fgwas/",anno_to_try,"-",sumstat_filename_fgwas_compatible,
                   " -w ", paste(collapse='+', annos_to_try),"+",anno_to_try ))
    llks_files <- list.files("out/fgwas/", pattern="*.llk")
    llks <- fread(llks)[1,2] # Get ln(likelihood) output
    names(llks) <- sub("-.*","",llks) # Should just be the anno names again. # TODO: could go wrong if user puts dashes in their anno names.
    #stopifnot(all(annos_to_try %in% names(llks)))
    if(max(llks) > best_llk) {
      new_best_llk <- max(llks)
      new_anno_acc <- c(anno_acc, names(llks)[which.max(llks)])
      return( getBestFgwasModel(sumstat_filename_fgwas_compatible, new_anno_acc, new_best_llk) )
    } else {return(anno_acc)}
  })
}
getBestFgwasModelP1_tail_recursive <- tailr::loop_transform(getBestFgwasModelP1_recursive)

# Iterative version
#mapply(sumstats, names(sumstats), SIMPLIFY=F, FUN=function(s,s_nm) {
#  # Test each anno individually; find that which most improves likelihood, add, try each remaining anno indiv, repeat.
#})

# P2 tests which penalty has the best x-validation likelihood, given the model from P1.
getBestFgwasModelP2 <- function(sumstat_filename_fgwas_compatible, annos) {
  # TODO: IDK the stats theory well enough: will the x-validation likelihoods here have only one minimum? If so, could optimize by doing a binary search for the best penalty or s/t.
  # TODO: either way, would be good to focus down on a minimum a little more after the best one in steps of 0.05 is found.
  xv_llks <- lapply(seq(0,1,0.05), function(penalty) {
    system(paste0( "../third_party/fgwas/src/fgwas -i ", sumstat_filename_fgwas_compatible,
                   " -w ", paste(collapse='+', annos),
                   " -p ", penalty, " -xv -print ",
                   " -o out/fgwas/p",as.character(penalty),"-",sumstat_filename_fgwas_compatible ))
    a <- fread(paste0( "out/fgwas/p",as.character(penalty),"-",sumstat_filename_fgwas_compatible,".ridgeparams"))
    xv_llk <- a[,nrow(a)] # fgwas outputs the x-validation likelihood from -xv as the last line of the ridgeparams files. # TODO: is it log-likelihood or just likelihood?
  })
  return( list(max(xv_llks), seq(0,1,0.05)[which.max(xv_llks)]) )
}

# P3 drops annos from the model using P2's penalty, until the x-validation likelihood is maximized.
getBestFgwasModelP3 <- function(sumstat_filename_fgwas_compatible, annos, xv_penalty, og_xv_llk) {
  # TODO: IDK the stats theory well enough: should I try every permutation of model? Or if dropping an anno increase the x-validation likelihood of the model, it will still do so in the context of any other annotations?
    # Here I assume that it is safe to just traverse the annotations once, since that's what the fgwas manual seems to do.
  xv_llks <- sapply(annos, function(anno_to_drop) {
    model_with_one_anno_dropped <- paste(collapse='+', annos[!anno_to_drop])
    system(paste0( "../third_party/fgwas/src/fgwas -i ", sumstat_filename_fgwas_compatible,
                   " -w ", model_with_one_anno_dropped,
                   " -p ", penalty, " -xv -print ",
                   " -o out/fgwas/drop_",anno_to_drop,"-",sumstat_filename_fgwas_compatible ))
    a <- fread(paste0( "out/fgwas/drop_",anno_to_drop,"-",sumstat_filename_fgwas_compatible ))
    xv_llk <- a[,nrow(a)]
  })
  names(xv_llks)[xv_llks >= og_xv_llk]
}

lapply(sumstats, function(s) {
  s <- s[, .SD, .SDcols=c( "rsid","chr","pos","z","se","SEGNUMBER",anno_config$name)]
  setnames(s,           c("SNPID","CHR","POS","Z","SE","SEGNUMBER",anno_config$name))
                                   # fgwas requires specific header names.
  s <- s[order(SEGNUMBER,CHR,POS)] # fgwas requires ordering by SEGNUMBER (if -fine option), and chr and pos.
  s[,chr := paste0("chr",CHR)]     # fgwas uses hg19 "chr#" syntax, not just the number.
  s[,`:=`(F=0, N=0)]               # fgwas errors if missing F or N col, even though overridden by SE.

  input_filename <- paste0("in/fgwas/",s_nm,"-fgwas.gz")
  fwrite(s, input_filename, sep=' ', compress="gzip")

  annos   <- getBestFgwasModelP1_tail_recursive(input_filename, anno_config$name, c(), 0)
  l       <- getBestFgwasModelP2(input_filename, annos)
  xv_llk  <- l[[1]]
  penalty <- l[[2]]

             getBestFgwasModelP3(input_filename, annos, penalty, xv_llk)
})
```

## Subset
Split the summary stats into sets of subsets, one per locus.
```{r}
sumstats <- lapply(sumstats, '[', order(rsid)) # Things remain sorted after subsetting.
subset_sumstatss <- lapply(1:nrow(loci_config), function(r) {
                    mapply(sumstats, data_config$ref_genome, SIMPLIFY=F, FUN = function(s, ref_genome) {

  s[ chr == loci_configs[[ref_genome]][r,chr]     &
     pos >= loci_configs[[ref_genome]][r,pos_min] &
     pos <= loci_configs[[ref_genome]][r,pos_max] ]
})})
names(subset_sumstatss) <- loci_config$locus
```

# LD
Calculate LD per ancestry per locus.\
This is more space-efficient than calculating LD per dataset per locus, assuming the datasets have most of their variants in common (which they should!—or else what is the point of multi-dataset finemapping?).
```{r}
# TODO: consider: generating subsets of sumstats and holding them all in memory? Or just holding the rsids necessary to subset each sumstat _later on_?
  # Could actualy be more concise to have just the rsids and subset as I go.

# Signed Pearson correlation.
# Returned LD matrix's rows/cols are sorted to match the order of the given rs_ids.
# Filters on the GDS file ~are~ taken into account. The GDS file will be returned with the same filters as before.
ldCalc <- function(gds_file, rs_ids, sample_ids=NULL, ref_alleles=NULL, outfile_name=NULL, n_threads=parallel::detectCores()) {
  if(is.null(ref_alleles)) warning("calc_ld() warning: specifying your data's ref_alleles is highly recommended! If the reference alleles (i.e. non-effect alleles) in your data =/= those in the reference panel, the LD will be incorrect.")
  seqFilterPush(gds_file) # Save user's prexisting filter on the file if they have one

  seqSetFilterAnnotID(gds_file, rs_ids)
  if(!is.null(sample_ids)) seqSetFilter(gds_file, sample.id = sample_ids, action="intersect")

  tmp_filename <- "/tmp/snpgds_format_file.gds"
  seqGDS2SNP(gds_file, tmp_filename, compress.geno="", compress.annotation="")
  tmp <- snpgdsOpen(tmp_filename, readonly=F)
  #print(paste(identical(read.gdsn(index.gdsn(tmp,"snp.rs.id")), seqGetData(gds_file,"annotation/id")))) # TRUE

  # Sort user-given ref_alleles to match the order of the GDS file before allele switching!
  # Why not sort the GDS file to match user input instead? B/c raw data files should not be edited.
  ref_alleles <- ref_alleles[match(seqGetData(gds_file,"annotation/id"), rs_ids)]
  snpgdsAlleleSwitch(tmp, toupper(ref_alleles))
  ld <- snpgdsLDMat(tmp, slide=0, method="corr", num.thread=n_threads)$LD

  snpgdsClose(tmp) # Don't need SNP GDS format file anymore
  unlink(tmp_filename)

  # Sort LD matrix to match the order of the user-given rs_ids.
  rs_ids_order <- match(rs_ids, seqGetData(gds_file,"annotation/id"))
  ld <- ld[rs_ids_order, rs_ids_order]

  if(!is.null(outfile_name)) {
    if(!dir.exists(dirname(outfile_name))) dir.create(dirname(outfile_name), recursive=T)
    fwrite(ld, outfile_name, sep=' ', col.names=F, compress="gzip")
  }

  seqFilterPop(gds_file) # Set filter to however it was before
  ld
}
# Note: it doesn't (and shouldn't!) matter that the seqSetFilterChrom filter from before is still active.

ld_rsid_orders <-
  sapply(       loci_config$locus,     simplify=F, function(l) {
  sapply(unique(data_config$ancestry), simplify=F, function(anc) {
    samples_of_an_ancestry <- samples_of_each_ancestry[[anc]]
    sumstats_of_an_ancestry <- subset_sumstatss[[l]][which(data_config$ancestry == anc)]
    common_rsids_in_sumstats_of_an_ancestry <- Reduce(union, lapply(sumstats_of_an_ancestry, '[', j=rsid))

    # TODO: not generalizable if multiple datasets in an ancestry with different rsids and/or different effect alleles.
      # Simple solution would be in data pre-proc: just flip alleles & β & z & etc..
    # For now w/ just the DIAMANTE data w/ one dataset per ancestry it's fine. Even the different ancestries have matching a0/a1, I checked.
    a_sumstat_of_an_ancestry <- sumstats_of_an_ancestry[[1]][rsid %in% common_rsids_in_sumstats_of_an_ancestry]
    #print(paste(identical(a_sumstat_of_an_ancestry$rsid, common_rsids_in_sumstats_of_an_ancestry)))
    ref_alleles <- a_sumstat_of_an_ancestry$a0

    ldCalc(ref, rs_ids = common_rsids_in_sumstats_of_an_ancestry,
             sample_ids = samples_of_an_ancestry,
            ref_alleles = ref_alleles,
           outfile_name = paste0("in/ld/",l,"-",anc,".ld.gz"),
              n_threads = 6L) 
  
    common_rsids_in_sumstats_of_an_ancestry # Return the order of the LD mat's rows/cols, important!
})})

save(ld_rsid_orders, file="in/ld/ld_rsid_orders.RData") # TODO: Save as separate files, with one col of just rsids? This way, LD could be used by non-R programs more easily.
```

# Run Fine-Mapping Methods
## SuSiEx
### Prep Input
SuSiEx is a command-line python program. It expects the following:\
A summary stats file, as follows:\ 
TODO: make nice table\
The β column _must_ be named "BETA" or "OR", but besides that they may be named arbitrarily.
```{r}
if(!dir.exists("in/susiex")) dir.create("in/susiex")
# TODO: ensure data contains the required columns
lapply(loci_config$locus, function(l) {
mapply(subset_sumstatss[[l]], names(sumstats), FUN=function(s, nm) {
  s <- s[, .(chr,pos,rsid,a0,a1,b,se,p)]
  setnames(s, old="b", new="BETA", skip_absent=T)
  fwrite(s, paste0("in/susiex/",l,"-",nm), sep='\t')
})})
```

```{r}
# TODO: PLINK stuff :(
  # NOTE TO SELF: see daily note 230605. Letting SuSiEx recompute its own LD for now just to get things working.
```

### Run SuSiEx
```{r}
if(!dir.exists("out/susiex")) dir.create("out/susiex", recursive=T)
err <- lapply(1:nrow(loci_config), function(r) {
  system(paste("../third_party/SuSiEx/bin/SuSiEx",
               "--sst_file", paste(collapse=',', paste0("in/susiex/",loci_config[r,locus],"-",names(sumstats))),
               "--n_gwas",   paste(collapse=',', data_config$n),
               "--ld_file",  paste(collapse=',', paste0("in/ld/",loci_config[r,locus],"-",unique(data_config$ancestry),"-susiex")), # TODO: compute own LD
               "--ref_file ../data/ref/1kg/plink_format/eas/g1000_eas,../data/ref/1kg/plink_format/eur/g1000_eur,../data/ref/1kg/plink_format/sas/g1000_sas", # TODO:
               "--plink plink", # TODO:
               "--out_dir out/susiex",
               "--out_name", loci_config[r,locus],
               #"--level 0.95", # Default
               "--chr ", loci_config[r,chr],
               "--bp ", paste(sep=',', loci_config[r,pos_min], loci_config[r,pos_max]),
               "--chr_col", paste(collapse=',', rep(1,nrow(data_config))),
               "--bp_col",  paste(collapse=',', rep(2,nrow(data_config))),
               "--snp_col", paste(collapse=',', rep(3,nrow(data_config))),
               "--a1_col",  paste(collapse=',', rep(4,nrow(data_config))),
               "--a2_col",  paste(collapse=',', rep(5,nrow(data_config))),
               "--eff_col", paste(collapse=',', rep(6,nrow(data_config))),
               "--se_col",  paste(collapse=',', rep(7,nrow(data_config))),
               "--pval_col",paste(collapse=',', rep(8,nrow(data_config))),
               # "--maf 0.05", # Default. TODO: 0 is not allowed, but I'd like to understand why, conceptually. Maybe just b/c in their code the filter is >= instead of >, so "--maf 0" would include 0 which is not good.
               #"--min_purity 0.5", # Default. Even if some credsets are impure and not practically useful, still want all the posteriors for comparison purposes. TODO: Purity filter of 0 causes errors for some loci, see daily note 230621. 0.01 is totally low enough but it not being 0 still bothering me. Low priorty to look into.
               "--mult-step True",
               "--keep-ambig True",
               "--threads", 6L # TODO: make more general, or allow input
               # defaults: --max_iter=100, --pval_thresh=1e-5, --tol=1e-4, --n_sig=5
  ))
})
# TODO: use "err" to print a warning if SuSiEx fails for some loci, and specify the loci.
```

## PAINTOR
### Prep Input
PAINTOR only works on variants shared by ALL input datasets.\
TODO: desc
```{r}
if(!dir.exists("in/paintor")) dir.create("in/paintor")
# input.files
writeLines(loci_config$locus, "in/paintor/input.files")

# Locus files. Format:
# rsid  pop1_z  pop2_z ...
#  rs1   -3.14  -1.337
#  ... <no NAs allowed>
desired_rsidss <- sapply(loci_config$locus, simplify=F, function(l) {
  signif_rsids <- Reduce(union,     lapply(subset_sumstatss[[l]], '[', j=rsid)) # NOTE: optional hardcoded p threshold here
  common_rsids <- Reduce(intersect, lapply(subset_sumstatss[[l]], '[',           j=rsid))
  desired_rsids <- intersect(signif_rsids, common_rsids)
})

lapply(loci_config$locus, function(l) {
  desired_rsids <- desired_rsidss[[l]]
  desired_sumstats <- lapply(subset_sumstatss[[l]], function(s) s[rsid %in% desired_rsids]) # We sorted before, so all sumstats' rsids are in the same order even after subsetting. 

  paintor_locus_file <- Reduce(cbind, lapply(desired_sumstats, '[', j=z), desired_rsids) # Can add more metadata than just rsid if you want
  colnames(paintor_locus_file) <- c("rsid", paste0(names(sumstats),"_z"))
  fwrite(paintor_locus_file, paste0("in/paintor/",l), sep=' ')
})

# LD files. Format: Pearson r, space-delim'd, no header, Must have same number of rows and order as locus file.
lapply(loci_config$locus, function(l) {
  desired_rsids <- desired_rsidss[[l]]
  ld_filenames <- paste0("in/ld/",l,"-",unique(data_config$ancestry),".ld.gz")
  desired_filenames <- paste0("in/paintor/",l,".ld_",unique(data_config$ancestry))
  lds <- lapply(ld_filenames, function(f) as.matrix(fread(f)))
  mapply(lds, ld_rsid_orders[[l]], desired_filenames, SIMPLIFY=F, FUN = function(ld, ld_rsid_order, nm) {
    # return(c(length(ld_rsid_order),length(desired_rsids), sum(ld_rsid_order %in% desired_rsids)))
    # TODO: This code could definitely be made clearer.
    desired_subset <- ld_rsid_order %in% desired_rsids # TODO: rename ld_rsid_order variable to not "order". Because it's just rsids.
    ld <- ld[desired_subset, desired_subset]
    ld_rsid_order <- ld_rsid_order[desired_subset]

    desired_order <- match(desired_rsids, ld_rsid_order)
    ld <- ld[desired_order,desired_order]

    fwrite(ld, nm, sep=' ', col.names=F)
  })
}) # TODO: A more elegant way? Unfortunately PAINTOR insists having all its input files in a single dir, so that means either cluttering in/ld/, changing filesnames in in/ld/ to use weird naming conventions, or making copies.... And PAINTOR doesn't accept compression so copies it is I guess....
# AND it wants to take in all loci at once, which means you have to store ALL the (decompressed!) LD files at once w/o being able to delete them as you go.

# Annotation files. Format:
# annot1  annot2 ...
# 0 or 1  0 or 1 
# Must have same number of rows and order as locus file.
# TODO: dummy all-0s file for now
lapply(loci_config$locus, function(l) {
  desired_rsids <- desired_rsidss[[l]]
  writeLines(c("dummy_annot", rep(0,length(desired_rsids))),
             paste0("in/paintor/",l,".annotations"))
})
```

### Run PAINTOR
```{r}
if(!dir.exists("out/paintor")) dir.create("out/paintor", recursive=T)
system(paste("../third_party/PAINTOR_V3.0/PAINTOR",
               "-input in/paintor/input.files",
               "-in in/paintor/",
               "-out out/paintor/",
               "-Zhead",  paste(collapse=',', paste0(names(sumstats),"_z")),
               "-LDname", paste(collapse=',', paste0("ld_",data_config$ancestry)),
               "-annotations dummy_annot",
               "-enumerate 1"
))
```
